\documentclass[12pt, letterpaper]{article}

\usepackage{amsmath}
\usepackage[a4paper, portrait, margin=1in]{geometry}
\usepackage{hyperref}
\usepackage{minted}
\usepackage{mdframed}
\usepackage{tikz}
\usepackage{listings}
\usepackage{multirow}
\usepackage{bookmark}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{framed}

\definecolor{shadecolor}{RGB}{240,240,240}
\newcommand{\mybox}[1]{\par\noindent\colorbox{shadecolor}
{\parbox{\dimexpr\textwidth-2\fboxsep\relax}{#1}}}

\renewcommand{\thesection}{Question \arabic{section}}
\renewcommand{\thesubsection}{\arabic{section}\alph{subsection})}
\renewcommand{\thesubsubsection}{\roman{subsubsection})}
\renewcommand{\thefigure}{\arabic{figure}}

\lstset{
basicstyle=\small\ttfamily,
columns=flexible,
breaklines=true
}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=blue,
}

\setlength{\parindent}{0em}
\setlength{\parskip}{1em}


\title{CSS601 Introduction to Artificial Intelligence - Assignment 2}
\author{Ian Chong Wei Ming\\
\href{mailto:ian.chong.2020@mitb.smu.edu.sg}{ian.chong.2020@mitb.smu.edu.sg}\\
+65-9680-8118}
\date{\today}

\begin{document}

\maketitle

\mybox{\textbf{Note:} Given information will be shaded \underline{grey} or in \textit{tables}.}

\section{Heuristic Search}
\begin{center}
    \begin{tikzpicture}[node distance=3.5cm]
        
        \node[circle,draw,label={\textcolor{red}{15}}] (S) {$S$};
        \node[circle,draw, below of=S] (B) {$B$};
        \node[circle,draw, left of=B] (A) {$A$};
        \node[circle,draw, right of=B] (C) {$C$};
        \node[circle,draw, below of=B] (D) {$D$};
        \node[circle,draw, below of=A] (E) {$E$};
        \node[circle,draw, below of=C] (F) {$F$};
        \node[circle,draw, below of=D] (H) {$H$};
        \node[circle,draw, below of=H] (G) {$G$};
    
        \path [->,draw] 
        (S) -- (B)
        (S) -- (A)
        (S) -- (C)
        (A) -- (B)
        (A) -- (E)
        (A) -- (H)
        (B) -- (C)
        (B) -- (D)
        (C) -- (D)
        (C) -- (F)
        (E) -- (H)
        (D) -- (H)
        (F) -- (H)
        (H) -- (G);
    
        \path[->,draw]
        (S) edge node[right, near end] {(3,\textcolor{red}{11})} (B)
        (S) edge node[left, near end] {(7,\textcolor{red}{13})} (A)
        (S) edge node[right, near end] {(8,\textcolor{red}{8})} (C)
        (A) edge node[above, near end] {(2,\textcolor{red}{11})} (B)
        (A) edge node[left, near end] {(6,\textcolor{red}{7})} (E)
        (A) edge node[right] {(10,\textcolor{red}{2})} (H)
        (B) edge node[above] {(4,\textcolor{red}{8})} (C)
        (B) edge node[left, near end] {(7,\textcolor{red}{5})} (D)
        (C) edge node[right, near end] {(1,\textcolor{red}{5})} (D)
        (C) edge node[right, near end] {(6,\textcolor{red}{7})} (F)
        (E) edge node[left, near end] {(5,\textcolor{red}{2})} (H)
        (D) edge node[right, near end] {(4,\textcolor{red}{2})} (H)
        (F) edge node[right, near end] {(4,\textcolor{red}{2})} (H)
        (H) edge node[right, near end] {3} (G);
    \end{tikzpicture}
\end{center}

\mybox{For the above graph (heuristic values are provided in red color and actual costs are in black
color), please provide answers to the following answers. Also note that S is start state and G
is goal state.}

\subsubsection{Is the heuristic admissible?}

\textbf{Response:} A heuristic is defined as \textit{admissible} if it never \textit{overestimates} the \textbf{cost} of reaching the \textbf{goal}. 
In otherwords, the cost that estimated by the heuristic to reach the goal \textbf{cannot} be \textit{higher} than the \textit{lowest} possible cost from the \textit{current point} in the path.
More formally, $h(n)\leq c(n,g) \forall n$

The minimum cost and the heuristic $h(n)$ to reach the goal from each node is given in the following table.
Workings will follow the table.

\begin{center}
    \begin{tabular}{|c|c|c|c|} 
    \hline
    Node $n$ & $c(n,g)$ & $h(n)$ & $h(n)\leq c(n,g)?$ \\ 
    \hline
    S & 15 & 15 & TRUE\\
    \hline
    A & 13 & 13 & TRUE\\
    \hline
    B & 12 & 11 & TRUE\\
    \hline
    C & 8 & 8 & TRUE\\
    \hline
    E & 8 & 7 & TRUE\\
    \hline
    D & 7 & 5 & TRUE\\
    \hline
    F & 7 & 7 & TRUE\\
    \hline
    H & 3 & 2 & TRUE\\
    \hline
    \end{tabular}
\end{center}

\textbf{Node S}

The path to follow would be $S\rightarrow B\rightarrow C\rightarrow D\rightarrow H\rightarrow G$

$c(n,g)=3+4+1+4+3=\underline{\textbf{15}}$

$h(n)=\textcolor{red}{15}$

\textbf{Node A}

The path to follow would be $A\rightarrow H\rightarrow G$

$c(n,g)=\underline{\textbf{13}}$

$h(n)=\textcolor{red}{13}$

\textbf{Node B}

The path to follow would be $B\rightarrow C\rightarrow D\rightarrow H\rightarrow G$

$c(n,g)=4+4+1+3=\textbf{\underline{12}}$

$h(n)=\textcolor{red}{11}$

\textbf{Node C}

The path to follow would be $C\rightarrow D\rightarrow H\rightarrow G$

$c(n,g)=1+4+3=\underline{\textbf{8}}$

$h(n)=\textcolor{red}{8}$

\textbf{Node D}

The path to follow would be $D\rightarrow H\rightarrow G$

$c(n,g)=4+3=\underline{\textbf{7}}$ 

$h(n)=\textcolor{red}{5}$

\textbf{Node E}

The path to follow would be $E\rightarrow H\rightarrow G$

$c(n,g)=5+3=\underline{\textbf{8}}$ 

$h(n)=\textcolor{red}{7}$

\textbf{Node F}

The path to follow would be $F\rightarrow H\rightarrow G$

$c(n,g)=4+3=\underline{\textbf{7}}$ 

$h(n)=\textcolor{red}{7}$

\textbf{Node H}

The path to follow would be $H\rightarrow G$

$c(n,g)=\underline{\textbf{3}}$ 

$h(n)=\textcolor{red}{2}$

Since $h(n)\leq c(n,g) \forall n$, the heuristic is \textit{admissible}.

\subsubsection{Is the heuristic consistent? Provide justification}
\textbf{Response:} No, the heuristic is not \textit{consistent}.

A heuristic is \textit{consistent} only if 

a) It is \textit{admissible} and;

b) Its estimate is \underline{always} less than or equal to the estimated distance from any neighbouring vertex to the goal, plus the cost of reaching that neighbour. More formally:

\[h(n) \leq c(n,p) + h(p) \forall n\]

Non-consistent nodes in the following table are coloured \colorbox{red}{RED}

\begin{center}
    \begin{tabular}{|c|c|c|c|} 
    \hline
    Node $n$ & $h(n)$ & $p$ & $c(n,p)+h(p)$ \tabularnewline
    \hline
    S & 15 & A & 20\tabularnewline
    \hline
    \rowcolor{red}S & 15 & B & 14\tabularnewline
    \hline
    S & 15 & C & 16\tabularnewline
    \hline
    A & 13 & B & 13\tabularnewline
    \hline
    A & 13 & E & 13\tabularnewline
    \hline
    \rowcolor{red}A & 13 & H & 12\tabularnewline
    \hline
    B & 11 & C & 12\tabularnewline
    \hline
    B & 11 & D & 12\tabularnewline
    \hline
    \rowcolor{red}C & 8 & D & 6\tabularnewline
    \hline
    C & 8 & F & 13\tabularnewline
    \hline
    D & 5 & H & 6\tabularnewline
    \hline
    E & 7 & H & 7\tabularnewline
    \hline
    \rowcolor{red}F & 7 & H & 6\tabularnewline
    \hline
    H & 2 & G & 3\tabularnewline
    \hline

    \end{tabular}
\end{center}

\subsubsection{DFS, BFS, Best First Search and A* Search}
\mybox{Provide the search steps (as discussed in class) with DFS, BFS (with and without priority
queue), Best First Search and A* search. Specify for each algorithm if the open list is
queue, stack, or priority queue. Feel free to add rows/columns, or other details in the
table below. Open list contains nodes that are to be explored, and “Nodes to add” are
the successors of the node that is recently popped.}

\textbf{Response:} The following tables detail the steps each algorithm takes.

\begin{center}
    \begin{tabular}{|c|c|c|c|}
    \hline
    \multicolumn{4}{|c|}{Depth First Search (\textbf{DFS})\textcolor{red}{*}}\\
    \hline
    Step No. & Open List & POP & Nodes to add \\ 
    \hline
    1 & $S$ & $S$ & $A^S, B^S, C^S$ \\
    \hline
    2 & $A^S, B^S, C^S$ & $A^S$ & $E^A,H^A$ \\
    \hline
    3 & $E^A, H^A, B^S, C^S$ & $E^A$ & nil \\
    \hline
    4 & $H^A, B^S, C^S$ & $H^A$ & $G^H$ \\
    \hline
    5 & $G^H,B^S,C^S$ & $G^H$ & nil \\
    \hline
    \end{tabular}
\end{center}

\textcolor{red}{*Open List is a \textit{stack}}

The path to follow is $S\rightarrow A\rightarrow H\rightarrow G$

Cost = 7 + 10 + 3 = \underline{\textbf{20}}

\begin{center}
    \begin{tabular}{|c|c|c|c|}
    \hline
    \multicolumn{4}{|c|}{Breadth First Search (\textbf{BFS})}\\
    \hline
    Step No. & Open List\textcolor{red}{*} & POP & Nodes to add \\ 
    \hline
    1 & $S$ & $S$ & $A^S, B^S, C^S$ \\
    \hline
    2 & $A^S, B^S, C^S$ & $A^S$ & $E^A,H^A$ \\
    \hline
    3 & $B^S, C^S, E^A, H^A$ & $B^S$ & $D^B$ \\
    \hline
    4 & $C^S, E^A, H^A, D^B$ & $C^S$ & $F^C$ \\
    \hline
    5 & $E^A,H^A,D^B,F^C$ & $E^A$ & nil \\
    \hline
    6 & $H^A,D^B,F^C$ & $H^A$  & $G^H$ \\
    \hline
    7 & $D^B,F^C,G^H$ & $D^B$  & nil \\
    \hline
    8 & $F^C,G^H$ & $F^C$  & nil \\
    \hline
    9 & $G^H$ & $G^H$  & nil \\
    \hline
    \end{tabular}
\end{center}

\textcolor{red}{*Open List is a \textit{queue}}

The path to follow is $S\rightarrow A\rightarrow H\rightarrow G$

Cost = 7 + 10 + 3 = \underline{\textbf{20}}

\begin{center}
    \begin{tabular}{|c|c|c|c|}
    \hline
    \multicolumn{4}{|c|}{Breadth First Search (\textbf{BFS} - Optimal)}\\
    \hline
    Step No. & Open List\textcolor{red}{*} & POP & Nodes to add \\ 
    \hline
    1 & $S$ & $S$ & $B^{S,3}, A^{S,7}, C^{S,8}$\\
    \hline
    2 & $B^{S,3}, A^{S,7}, C^{S,8}$ & $B^{S,3}$ & $C^{B,7},D^{B,10}$ \\
    \hline
    3 & $A^{S,7}, C^{B,7}, D^{B,10}$ & $A^{S,7}$ & $E^{A,13}, H^{A,17}$ \\
    \hline
    4 & $C^{B,7}, D^{B,10}, E^{A,13}, H^{A,17}$ & $C^{B,7}$ & $D^{C,8}, F^{C,13}$\\
    \hline
    5 & $D^{C,8}, E^{A,13}, F^{C,13}, H^{A,17}$ & $D^{C,8}$ & $H^{D,12}$\\
    \hline
    6 & $H^{D,12}, E^{A,13}, F^{C,13}$ & $H^{D,12}$ & $G^{H,15}$\\
    \hline
    7 & $E^{A,13}, F^{C,13}, G^{H,15}$ & $E^{A,13}$ & $H^{E,18}$\textcolor{red}{**}\\
    \hline
    8 & $F^{C,13}, G^{H,15}$ & $F^{C,13}$ & $H^{F,18}$\textcolor{red}{**}\\
    \hline
    9 & $G^{H,15}$ & $G^{H,15}$ & nil\\
    \hline
    \end{tabular}
\end{center}

\textcolor{red}{*Open List is a \textit{priority queue}}

\textcolor{red}{**not added to the Open list since it is already \textit{explored}}

The path to follow is $S\rightarrow B\rightarrow C\rightarrow D\rightarrow H\rightarrow G$

Cost = 3 + 4 + 1 + 4 + 3 = \underline{\textbf{15}}

\begin{center}
    \begin{tabular}{|c|c|c|c|}
    \hline
    \multicolumn{4}{|c|}{Best First Search}\\
    \hline
    Step No. & Open List\textcolor{red}{*} & POP & Nodes to add \\ 
    \hline
    1 & $S$ & $S$ & $A^{S,13}, B^{S,11}, C^{S,8}$ \\
    \hline
    2 & $C^{S,8}, B^{S,11}, A^{S,13}$ & $C^{S,8}$ & $D^{C,5}, F^{C,7}$ \\
    \hline
    3 & $D^{C,5}, F^{C,7}, B^{S,11}, A^{S,13}$ & $D^{C,5}$ & $H^{D,2}$ \\
    \hline
    4 & $H^{D,2}, F^{C,7}, B^{S,11}, A^{S,13}$ & $H^{D,2}$ & $G^{H,0}$\\
    \hline
    5 & $G^{H,0}, F^{C,7}, B^{S,11}, A^{S,13}$ & $G^{H,0}$ & nil\\
    \hline
    \end{tabular}
\end{center}

\textcolor{red}{*Open List is a \textit{priority queue}}

The path to follow is $S\rightarrow C\rightarrow D\rightarrow H\rightarrow G$

Cost = 8 + 1 + 4 + 3 = \underline{\textbf{16}}

\begin{center}
    \begin{tabular}{|c|c|c|c|}
    \hline
    \multicolumn{4}{|c|}{A* Search}\\
    \hline
    Step No. & Open List\textcolor{red}{*} & POP & Nodes to add \\ 
    \hline
    1 & $S$ & $S^{15}$ & $B^{S,14}, C^{S,16}, A^{S,20}$ \\
    \hline
    2 & $B^{S,14}, C^{S,16}, A^{S,20}$ & $B^{S,14}$ & $C^{B,15}, D^{B,15}$ \\
    \hline
    3 & $C^{B,15}, D^{B,15}, A^{S,20}$ & $C^{B,15}$ & $D^{C,13},F^{C,20}$ \\
    \hline
    4 & $D^{C,13}, A^{S,20}, F^{C,20}$ & $D^{C,13}$ & $H^{D,14}$\\
    \hline
    5 & $H^{D,14}, A^{S,20}, F^{C,20}$ & $H^{D,14}$ & $G^{H,15}$\\
    \hline
    6 & $G^{H,15}, A^{S,20}, F^{C,20}$ & $G^{H,15}$ & nil\\
    \hline
    \end{tabular}
\end{center}

\textcolor{red}{*Open List is a \textit{priority queue}}

The path to follow is $S\rightarrow B\rightarrow C\rightarrow D\rightarrow H\rightarrow G$

Cost = 3 + 4 + 1 + 4 + 3 = \underline{\textbf{15}}

\section{Build an automated car in a simulator}

\mybox{The goal of this assignment is to guide your car to move in a multi-lane straight road (represented using a rectangular grid) simulator (details explained later). 

In addition to your car, there are other cars also moving along the road. You need to design an algorithm which can take your car from its start position to the end of the road as soon as possible while avoiding other cars. 

Other cars are moving randomly. At any step, your car can view up to 4 cells in front, left and right, i.e., it will not be able to look at the other cars which are beyond its visibility range of 4 cells. (This visibility range of 4 is configurable and your algorithm should work irrespective of visibility range).

We describe the state, operators and goal state for this problem below.

\textbf{State}: State of the car is its location, i.e., the grid cell it is currently present in. 

\textbf{Operators/Actions}: Following are the available operators or actions for the car:

\begin{itemize}
    \item \textbf{Forward} – Moves the car one step ahead in the same lane.
    \item \textbf{Forward-2$\times$} – Moves the car two steps ahead in the same lane.
    \item \textbf{Forward-3$\times$} – Moves the car three steps ahead in the same lane.
    \item \textbf{Left} – Moves the car in the left lane and one step forward.
    \item \textbf{Right} – Moves the car in the right lane and one step forward.
    \item \textbf{None} – Stays at the same place.
\end{itemize}

If the action is unsuccessful then the car will stay at the same place. Action will be unsuccessful if there is a wall or another car present in the intended direction of move. For example, a forward will be unsuccessful if there is another car present in front of your car.
As the left action moves a car to the left and one step forward, a left action taken in grid cell (5,3) will be unsuccessful if there is a car in either of (4,3) or (4,4).

\textbf{Goal State:} There is one goal state, where your car needs to reach.}

\subsection{Write the function}
\mybox{Write the function that will generate the \textbf{operator/action} to be taken at the current time step (based on search algorithms discussed in class), so that time taken to reach goal is as \textit{minimal} as possible. 
[You may refer to “\texttt{pseudo code.pdf}” for implementation details and pseudo code for $A* Search$.]
The code for the Simulator is provided in the zip file \texttt{"SelfDrivingCar.zip"}. The details about the
installation/running of simulator and code details are provided in the file \texttt{"SelfDrivingCar.pdf"}
(present in the \texttt{.zip} file).}

\textbf{Response:} Please see attached \texttt{python} script.

\subsection{Submit code in \texttt{.pdf}}

\textbf{Response:}

\subsubsection{Algorithm 1 - \texttt{drive}}

This is the code for the main drive code. It depends on the subsequent helper functions.

\begin{mdframed}[backgroundcolor=shadecolor]
\begin{minted}[fontsize=\footnotesize,breaklines]{python}
def drive(self, goalstates, inputs):    
    
    path_list = []
    destination_reached = []
    action_order = []
    path_lengths = []
    start = self.state

    for goal in goalstates:
        goalReached, path = self.AStar(start["location"], goal, inputs)
        destination_reached.append(goalReached)
        path_list.append(path)
        path_lengths.append(len(path))
    
    if True in destination_reached:        
        best_path = [path_list[i] for i in range(len(path_list)) if (destination_reached[i] == True) and (len(path_list[i]) == min(path_lengths))]
        action_order.extend(best_path)
    else:        
        longest_path = [path_list[i] for i in range(len(path_list)) if (len(path_list[i]) == max(path_lengths))]
        action_order.extend(longest_path)

    movements = {
        (0, 3): "forward-3x", 
        (0, 2): "forward-2x", 
        (0, 1): "forward", 
        (-1, 1): "left", 
        (1, 1): "right", 
        (0, 0): None}
    try:
        action_sequence = [movements[(action_order[0][i+1][0] - action_order[0][i][0], action_order[0][i+1][1] - action_order[0][i][1])] for i in range(len(action_order[0])-1)]
    except:
        action_sequence = [None]
    
    return action_sequence
\end{minted}
\end{mdframed}

\subsubsection{Algorithm 2 - \texttt{AStar}}

Implementation of $A* Search$ to car to consider which action to take. Relies on \texttt{applyAction} to function properly.

\begin{mdframed}[backgroundcolor=shadecolor]
\begin{minted}[fontsize=\footnotesize,breaklines]{python}
def AStar(self, start, goal, state):    
    closedSet = []
    openSet = [start]
    cameFrom = {}
    gScore = {start: 0}
    fScore = {start: self.heuristic_cost_estimate(start, goal)}
    current = []
    
    while len(openSet) > 0:
        min_idx = list(fScore.values()).index(min(fScore.values()))
        current = list(fScore.keys())[min_idx]
        if current == goal["location"]:
            return True, self.reconstruct_path(cameFrom, current)
        del fScore[current]
        openSet.remove(current)
        closedSet.append(current)
        
        for action in ["forward-3x", "forward-2x", "forward", "left", "right"]:
            neighbor = self.applyAction(current, action, state, goal["location"])
            if neighbor == current:
                continue
            tentative_gScore = gScore[current] + 1
            if neighbor not in openSet:
                openSet.append(neighbor)
            elif tentative_gScore >= gScore[neighbor]:
                continue
            cameFrom[neighbor] = current
            gScore[neighbor] = tentative_gScore
            fScore[neighbor] = gScore[neighbor] + self.heuristic_cost_estimate(neighbor, goal)
        
    if current == goal["location"]:
        return True, self.reconstruct_path(cameFrom, current)
    else:
        return False, self.reconstruct_path(cameFrom, current)
\end{minted}
\end{mdframed}

\subsubsection{Algorithm 2.5 - \texttt{applyAction}}

Helper function to determine if a move is valid and apply it. For invalid moves, returns the current position \textit{i.e.} no action

\begin{mdframed}[backgroundcolor=shadecolor]
\begin{minted}[fontsize=\footnotesize,breaklines]{python}
def applyAction(self, current, action, state, goal):
    movements = {
        "forward-3x": [(0, 1), (0, 2), (0, 3)], 
        "forward-2x": [(0, 1), (0, 2)], 
        "forward": [(0, 1)], 
        "left": [(-1, 0), (-1, 1)], 
        "right": [(1, 0), (1, 1)], 
        None: [(0, 0)]}
    
    target_row = current[0]
    target_col = current[1]
    valid_move = True
    
    for movement in movements[action]:
        if (target_row + movement[0] >= len(state)) or \
            (target_row + movement[0] < 0) or \
            (target_col + movement[1]) >= len(state[0]):
            return current
                
        if state[target_row + movement[0]][target_col + movement[1]] == 1:
            valid_move = False
            break
        
    if abs(target_row + movements[action][-1][0] - goal[0]) > abs(goal[1] - (target_col + movements[action][-1][1])):
        return current
    
    if valid_move:
        target_row += movements[action][-1][0]
        target_col += movements[action][-1][1]
    return (target_row, target_col)
\end{minted}
\end{mdframed}

\subsubsection{Algorithm 3 - \texttt{heuristic\_cost\_estimate}}

Estimates the heuristic cost from current node.

\begin{mdframed}[backgroundcolor=shadecolor]
\begin{minted}[fontsize=\footnotesize,breaklines]{python}
def heuristic_cost_estimate(self, start, goal):
    result = abs(goal["location"][1] - start[1]) + abs(goal["location"][0] - start[0]) / 2
    return result
\end{minted}
\end{mdframed}

\subsubsection{Algorithm 4 - \texttt{reconstruct\_path}}

Helper function to reconstruct the path from start to current node.

\begin{mdframed}[backgroundcolor=shadecolor]
\begin{minted}[fontsize=\footnotesize,breaklines]{python}
def reconstruct_path(self, cameFrom, current):
    total_path = []
    while current in cameFrom.keys():
        total_path.append(current)
        current = cameFrom[current]
    total_path.append(current)
    total_path = total_path[::-1]
    return total_path
\end{minted}
\end{mdframed}

\section{Taxi Driver Markov Decision Process}
\mybox{Taxi drivers in Singapore can pick up customers from any location that is not on highways. 
However, they require guidance on where to pick up customers when they do not have customers on board and there are no bookings. 
In this question, we address this guidance problem. 
There are four locations: $L1$, $L2$, $L3$ and $L4$ from where taxi drivers can pick up and drop off customers. 
At any decision epoch, the chances of the taxi driver picking up a customer from different locations are provided in \textbf{Table 1} below:}

\begin{center}
    \textbf{Table 1}

    \begin{tabular}{|c|c|} 
    \hline
    Location & Chance of finding customer\\ 
    \hline
    $L1$ & 0.3 \\
    \hline
    $L2$ & 0.8 \\
    \hline
    $L3$ & 0.1 \\
    \hline
    $L4$ & 0.6 \\
    \hline
    \end{tabular}
    
\end{center}

\mybox{Once the taxi driver picks up a customer, the customer determines the destination. Observed
probabilities (from past data) of a customer starting from a source location and going to a
destination location are given below:}

\begin{center}
    \textbf{Table 2}

    \begin{tabular}{|c|c|} 
    \hline
    $Source \rightarrow Destination$ & Probability\\ 
    \hline
    $L1 \rightarrow L2$ & 0.4 \\
    \hline
    $L1 \rightarrow L3$ & 0.35 \\
    \hline
    $L1 \rightarrow L4$ & 0.25 \\
    \hline
    $L2 \rightarrow L1$ & 0.4 \\
    \hline
    $L2 \rightarrow L3$ & 0.6 \\
    \hline
    $L3 \rightarrow L1$ & 0.6 \\
    \hline
    $L3 \rightarrow L4$ & 0.4 \\
    \hline
    $L4 \rightarrow L1$ & 0.65 \\
    \hline
    $L4 \rightarrow L2$ & 0.35 \\
    \hline
    \end{tabular}    
\end{center}
\mybox{\textit{**If a source destination pair does not appear in the above table, it indicates the fare is 0.}

Cost of travelling between locations is as follows:}

\begin{center}
    \textbf{Table 3}

    \begin{tabular}{|c|c|} 
    \hline
    $Source \rightarrow Destination$ & Cost (\$)\\ 
    \hline
    $L1 \rightarrow L2$ & 1.00 \\
    \hline
    $L1 \rightarrow L3$ & 1.50 \\
    \hline
    $L1 \rightarrow L4$ & 1.25 \\
    \hline
    $L2 \rightarrow L1$ & 1.00 \\
    \hline
    $L2 \rightarrow L3$ & 0.75 \\
    \hline
    $L3 \rightarrow L1$ & 1.50 \\
    \hline
    $L3 \rightarrow L4$ & 0.80 \\
    \hline
    $L4 \rightarrow L1$ & 1.25 \\
    \hline
    $L4 \rightarrow L2$ & 1.00 \\
    \hline
    \end{tabular}
\end{center}
\textit{**If a source destination pair does not appear in the above table, it indicates the cost is infinity. If the taxi does not move (i.e., source and destination are same), then the cost is zero.}

\mybox{The taxi driver can either pickup from the current location or move to another location. 
Pickup corresponds to picking up a customer (if one is found) and dropping them of at their destination. 
Pickup action succeeds with probabilities specified in \textbf{Table 1} and if the taxi picks up a customer, destination location is determined by the probabilities in \textbf{Table 2}. 
When Pickup action fails, the taxi remains in its current location. 
Move to another location is always successful and taxi moves to the desired location with probability 1.
Both pickup and move actions take one-time step each. Please provide the following:}

\subsection{MDP Model for "move and pickup customers"}

\begin{center}
    \begin{tabular}{|c|l|c|r|r|}
    \hline
    s (current state)       &$a$ (action) & $s'$ (new state) & $P(s'|s,a)$ & $R(s,a,s')$ 
    \\ \hline
    \multirow{8}{*}{$L1$}   & Pick-up $\rightarrow$ $L1$ & $L1$ & $1-0.3=0.70$ & $0$ \\ \cline{2-5} 
                            & Pick-up $\rightarrow$ $L2$ & $L2$ & $0.3\times 0.4=0.12$ & $8-1=7$ \\ \cline{2-5} 
                            & Pick-up $\rightarrow$ $L3$ & $L3$ & $0.3\times 0.35=0.11$ & $13-1.5=11.5$ \\ \cline{2-5} 
                            & Pick-up $\rightarrow$ $L4$ & $L4$ & $0.3\times 0.25=0.075$ & $15-1.25=13.75$ \\ \cline{2-5}
                            & Move $\rightarrow$ $L1$ & $L1$ & $1$ & $0$ \\ \cline{2-5}
                            & Move $\rightarrow$ $L2$ & $L2$ & $1$ & $0-1=-1$ \\ \cline{2-5}
                            & Move $\rightarrow$ $L3$ & $L3$ & $1$ & $0-1.5=-1.5$ \\ \cline{2-5}
                            & Move $\rightarrow$ $L4$ & $L4$ & $1$ & $0-1.25=-1.25$ \\ \cline{2-5}
    \hline           
    \multirow{8}{*}{$L2$}   & Pick-up $\rightarrow$ $L1$ & $L1$ & $0.8\times0.4=0.32$ & $10-1=9$ \\ \cline{2-5}
                            & Pick-up $\rightarrow$ $L2$ & $L2$ & $1-0.8=0.2$ & $0$ \\ \cline{2-5} 
                            & Pick-up $\rightarrow$ $L3$ & $L3$ & $0.8\times 0.6=0.48$ & $9-0.75=8.25$ \\ \cline{2-5} 
                            & Pick-up $\rightarrow$ $L4$ & $L4$ & $0$ & $0-\infty=\infty$ \\ \cline{2-5}
                            & Move $\rightarrow$ $L1$ & $L1$ & $1$ & $0-1=-1$ \\ \cline{2-5}
                            & Move $\rightarrow$ $L2$ & $L2$ & $1$ & $0$ \\ \cline{2-5}
                            & Move $\rightarrow$ $L3$ & $L3$ & $1$ & $0-0.75=-0.75$ \\ \cline{2-5}
                            & Move $\rightarrow$ $L4$ & $L4$ & $1$ & $0-\infty=\infty$ \\ \cline{2-5}
    \hline
    \multirow{8}{*}{$L3$}   & Pick-up $\rightarrow$ $L1$ & $L1$ & $0.1\times 0.6=0.06 $ & $13-1.5=11.5$ \\ \cline{2-5}
                            & Pick-up $\rightarrow$ $L2$ & $L2$ & $0$ & $0-\infty=\infty$ \\ \cline{2-5} 
                            & Pick-up $\rightarrow$ $L3$ & $L3$ & $1-0.1=0.9$ & $0$ \\ \cline{2-5} 
                            & Pick-up $\rightarrow$ $L4$ & $L4$ & $0.1\times 0.4=0.04$ & $10-0.8=9.2$ \\ \cline{2-5}
                            & Move $\rightarrow$ $L1$ & $L1$ & $1$ & $0-1.5=-1.5$ \\ \cline{2-5}
                            & Move $\rightarrow$ $L2$ & $L2$ & $1$ & $0-\infty=\infty$ \\ \cline{2-5}
                            & Move $\rightarrow$ $L3$ & $L3$ & $1$ & $0$ \\ \cline{2-5}
                            & Move $\rightarrow$ $L4$ & $L4$ & $1$ & $0-0.8=-0.8$ \\ \cline{2-5}
    \hline
    \multirow{8}{*}{$L4$}   & Pick-up $\rightarrow$ $L1$ & $L1$ & $0.6\times 0.65=0.39 $ & $9-1.25=7.75$ \\ \cline{2-5}
                            & Pick-up $\rightarrow$ $L2$ & $L2$ & $ 0.6\times 0.35=0.21 $ & $7-1=6$ \\ \cline{2-5} 
                            & Pick-up $\rightarrow$ $L3$ & $L3$ & $0$ & $0-\infty=\infty$ \\ \cline{2-5} 
                            & Pick-up $\rightarrow$ $L4$ & $L4$ & $1-0.6=0.4$ & $0$ \\ \cline{2-5}
                            & Move $\rightarrow$ $L1$ & $L1$ & $1$ & $0-1.25=-1.25$ \\ \cline{2-5}
                            & Move $\rightarrow$ $L2$ & $L2$ & $1$ & $0-1=-1$ \\ \cline{2-5}
                            & Move $\rightarrow$ $L3$ & $L3$ & $1$ & $0-\infty=\infty$ \\ \cline{2-5}
                            & Move $\rightarrow$ $L4$ & $L4$ & $1$ & $0$ \\ \cline{2-5}
    \hline
    \end{tabular}
\end{center}

\subsection{Initialize $\forall s,V^0(s)=0$ and calculate:}

\textbf{Response:}

Computations are done in the following table, with figures rounded to 2 decimal places

\begin{footnotesize}
    \begin{tabular}{|l|c|c|c|c|c|c|c|c|c|c|c|}        
    \hline
    $s$ & $a$  & $V^0(s)$ & $Q^1(s,a)$ & $V^1(s)$ & $\pi^1(s)$ & $Q^2(s,a)$ & $V^2(s)$ & $\pi^2(s)$ & $Q^3(s,a)$ & $V^3(s)$ & $\pi^3(s)$ \\ \hline
    \multirow{5}{*}{$L1$}   & Pickup            & 0 & 3.08      & 3.08  & pickup & 6.49 & 6.49  & pickup & 9.80 & 9.80  & pickup      \\ \cline{2-12}
                            & $\rightarrow L1$  & 0 & 0.00      & 3.08  & pickup & 3.08 & 6.49  & pickup  & 6.49 & 9.80  & pickup   \\ \cline{2-12}
                            & $\rightarrow L2$  & 0 & -1.00     & 3.08  & pickup & 5.84 & 6.49  & pickup & 8.70 & 9.80  & pickup  \\ \cline{2-12}
                            & $\rightarrow L3$  & 0 & -1.50     & 3.08  & pickup & -0.44 & 6.49  & pickup & 1.98 & 9.80  & pickup   \\ \cline{2-12}
                            & $\rightarrow L4$  & 0 & -1.25     & 3.08  & pickup & 3.03 & 6.49  & pickup & 7.38 & 9.80  & pickup \\ \cline{2-12}
    \hline
    \multirow{5}{*}{$L2$}   & Pickup            & 0 & 6.48      & 6.48  & pickup & 9.70 & 9.70 & pickup & 12.53 & 12.53 & pickup \\ \cline{2-12}
                            & $\rightarrow L1$  & 0 & -1.00     & 6.48  & pickup & 2.08 & 9.70 & pickup & 5.49 & 12.53 & pickup \\ \cline{2-12}
                            & $\rightarrow L2$  & 0 & 0.00      & 6.48  & pickup & 6.84 & 9.70 & pickup & 9.70 & 12.53 & pickup \\ \cline{2-12}
                            & $\rightarrow L3$  & 0 & -.075     & 6.48  & pickup & 0.31 & 9.70 & pickup & 2.73 & 12.53 & pickup \\ \cline{2-12}
                            & $\rightarrow L4$  & 0 & -$\infty$ & 6.48  & pickup & -$\infty$ & 9.70 & pickup & -$\infty$ & 12.53  & pickup\\ \cline{2-12}
    \hline
    \multirow{5}{*}{$L3$}   & Pickup            & 0 & 1.06      & 1.06 & pickup & 2.37 & 3.48 & $\rightarrow L4$ & 4.93 & 7.83 & $\rightarrow L4$ \\ \cline{2-12}
                            & $\rightarrow L1$  & 0 & -1.50     & 1.06 & pickup & 1.58 & 3.48 & $\rightarrow L4$ & 4.99 & 7.83 & $\rightarrow L4$ \\ \cline{2-12}
                            & $\rightarrow L2$  & 0 & -$\infty$ & 1.06 & pickup & -$\infty$ & 3.48 & $\rightarrow L4$ & -$\infty$ & 7.83 & $\rightarrow L4$ \\ \cline{2-12}
                            & $\rightarrow L3$  & 0 & 0.00      & 1.06 & pickup & 1.06 & 3.48 & $\rightarrow L4$ & 3.48 & 7.83 & $\rightarrow L4$ \\ \cline{2-12}
                            & $\rightarrow L4$  & 0 & -0.80     & 1.06 & pickup & 3.48 & 3.48 & $\rightarrow L4$ & 7.83 & 7.83  & $\rightarrow L4$ \\ \cline{2-12}
    \hline
    \multirow{5}{*}{$L4$}   & Pickup            & 0 & 4.28      & 4.28  & pickup    & 8.63          & 8.63  & pickup    & 12.30     & 12.30 & pickup \\ \cline{2-12}
                            & $\rightarrow L1$  & 0 & -1.25     & 4.28  & pickup    & 1.82          & 8.63  & pickup    & 5.24      & 12.30 & pickup \\ \cline{2-12}
                            & $\rightarrow L2$  & 0 & -1.00     & 4.28  & pickup    & 5.84          & 8.63  & pickup    & 8.70      & 12.30 & pickup  \\ \cline{2-12}
                            & $\rightarrow L3$  & 0 & -$\infty$ & 4.28  & pickup    & -$\infty$     & 8.63  & pickup    & -$\infty$ & 12.30 & pickup  \\ \cline{2-12}
                            & $\rightarrow L4$  & 0 & 0.00      & 4.28  & pickup    & 4.28          & 8.63  & pickup    & 8.63      & 12.30 & pickup  \\ \cline{2-12}
    \hline
    \end{tabular}
\end{footnotesize}

\subsubsection{$\forall s,V^1(s),V^2(s),V^3(s)$}

\textbf{Calculations for $V^1$}
Key parameters for our MDP
$V_0(s) = 0, \gamma = 1$ 
$Q^t(s,a) = \sum_{s'} P(s'|s,a)[R(s,a,s')+ \gamma V_0(s')]$
$V^*(s)=max_aQ(s,a)$

\section{\texttt{OpenAI FrozenLake} environment}

\mybox{In this question, you need to install OpenAI gym (\href{http://gym.openai.com/docs/}{http://gym.openai.com/docs/}). 
Once the gym is installed, you have to implement \textit{Q-learning} for the \texttt{FrozenLake} environment (\href{https://gym.openai.com/envs/FrozenLake-v0/}{https://gym.openai.com/envs/FrozenLake-v0/}) in python using the gym library and show the rewards obtained. 
You may use a discount factor of \texttt{0.95}.

Please provide the following two things in your solution for this question:}

\subsection{Implement \textit{Q-learning}}
\mybox{Code that implements \textit{Q-learning} for \texttt{Frozenlake} example in \texttt{gym}. 
Copy and paste the code in the solution pdf, and provide the actual code file also.}

\textbf{Response:}

First, we import the relevant packages and initialize the \texttt{gym} \texttt{FrozenLake-v0} environment.

\begin{mdframed}[backgroundcolor=shadecolor]
\begin{minted}[fontsize=\footnotesize,breaklines]{python}
import numpy as np
import gym
import time
from matplotlib import pyplot as plt

env = gym.make("FrozenLake-v0")
\end{minted}
\end{mdframed}

We now set key parameters of our Q-Learning experiment

\begin{mdframed}[backgroundcolor=shadecolor]
\begin{minted}[fontsize=\footnotesize,breaklines]{python}
epsilon = 0.9
min_epsilon = 0.01
decay_rate = 0.9
total_episodes = 30000
step_limit = 100
learning_rate = 0.05
gamma = 0.95
\end{minted}
\end{mdframed}

We next intialize the environment and define helper functions to run our $FrozenLake$ simulation.

\begin{mdframed}[backgroundcolor=shadecolor]
\begin{minted}[fontsize=\footnotesize,breaklines]{python}
env = gym.make("FrozenLake-v0")
Q = np.zeros((env.observation_space.n, env.action_space.n))
steps_total = []
rewards_total = []
egreedy_total = []

def pick_action(observation):
    action = 0
    if np.random.uniform(0, 1) < epsilon:
        action = env.action_space.sample()
    else:
        action = np.argmax(Q[observation, :])
    return action

def learn(obs_old, obs_new, reward, action,gamma=0.95,learning_rate=0.1):
    prediction = Q[obs_old, action]
    target = reward + gamma * np.max(Q[obs_new, :])
    Q[obs_old, action] = Q[obs_old, action] + learning_rate * (target - prediction)

def movingaverage(values, window):    
    weights = np.repeat(1.0, window)/window
    sma = np.convolve(values, weights, 'valid')
    return sma
\end{minted}
\end{mdframed}

\subsection{Episode return}
\mybox{For each episode, compute the total accumulated reward (also called \textit{episode return}). Plot the average return (over the last 100 episodes) while your agent is learning (x-axis will be the episode number, y-axis will be the average return over the last 100 episodes). \textbf{Make sure that you train for sufficiently many episodes so that convergence occurs}.

The goal in this question is to ensure you familiarize yourself with \texttt{OpenAI} and understand how to implement \textit{Q-learning} in \texttt{OpenAI}. 
There are many resources available online on implementing \textit{Q-learning} in \texttt{OpenAI} and the right value of learning rate for the \texttt{FrozenLake} example.
You are free to refer to them, but please write your own code. 
Tabular \textit{Q-learning} should work in this question.}

We now run the simulation using the functions and parameters we have defined prior.

\begin{mdframed}[backgroundcolor=shadecolor]
\begin{minted}[fontsize=\footnotesize,breaklines]{python}
for episode in range(total_episodes):
    obs = env.reset()
    t = 0
    if episode % step_limit == 99:
        epsilon *= decay_rate
        epsilon = max(epsilon, min_epsilon)
    while t < step_limit:
        action = pick_action(obs)
        obs2, reward, done, info = env.step(action)
        learn(obs, obs2, reward, action, gamma,learning_rate)
        obs = obs2
        t += 1
        if done:            
            rewards_total.append(reward)
            egreedy_total.append(epsilon)
            steps_total.append(t)
            if reward > 0.0:
                outcome = "WIN"                
            else:
                outcome = "LOSE"
            print(f"Episode {episode} | {t} steps | {outcome}")
            break
\end{minted}
\end{mdframed}

We use the following code to plot the $MA100$ moving average for the episode return.

\begin{mdframed}[backgroundcolor=shadecolor]
\begin{minted}[fontsize=\footnotesize,breaklines]{python}
window = 100
average_reward = movingaverage(rewards_total, window)

plt.figure(dpi=150)
plt.title(f"Episode return SMA[{window}]")
plt.ylabel("Reward")
plt.xlabel("Episode")
plt.plot(average_reward)
plt.savefig("episode_return.jpg")
\end{minted}
\end{mdframed}

\begin{figure}[hbt!]
    \caption{Episodic Return for $FrozenLake$ simulation}
    \centering
    \includegraphics{episode_return.jpg}
\end{figure}

\section{\texttt{200Reviews.csv} SVD}

\mybox{In this question, you will employ \textit{Singular Value Decomposition} to obtain word embeddings and compare the generated word embeddings with the word embeddings generated using \texttt{word2vec}. 
The corpus (or dataset) to be considered is "\texttt{200Reviews.csv}". You need to do the following:}

\subsection{Parsing data}
\mybox{Parse the reviews in "\texttt{200Reviews.csv}", i.e., divide reviews into sentences and sentences into words and remove the stop words. You can employ the "\texttt{NLP-pipeline-example.ipynb}" example we talked about in class.}

\textbf{Response:} We import the packages required to process text in our preamble and read the dataframe as shown.

\begin{mdframed}[backgroundcolor=shadecolor]
\begin{minted}[fontsize=\footnotesize,breaklines]{python}
# Importing of respective packages that will be used for the processing
import pandas as pd
import numpy as np
import re

import nltk
from nltk.stem import WordNetLemmatizer
from nltk.corpus import wordnet
from nltk.corpus import stopwords
from bs4 import BeautifulSoup

import warnings
warnings.filterwarnings('ignore')
\end{minted}
\end{mdframed}

{\large Text Lemmatization}

As we are using \texttt{WordNet} Lemmatizer and the the standard \texttt{nltk} \texttt{pos} tags are \texttt{treebank} tags, we need to convert the \texttt{treebank} tag to \texttt{WordNet} tags.

We write the following functions to implement our lemmatization pipeline:

\begin{mdframed}[backgroundcolor=shadecolor]
\begin{minted}[fontsize=\footnotesize,breaklines]{python}
def get_wordnet_pos(treebank_tag):
    if treebank_tag.startswith('J'):
        return wordnet.ADJ
    elif treebank_tag.startswith('V'):
        return wordnet.VERB
    elif treebank_tag.startswith('N'):
        return wordnet.NOUN
    elif treebank_tag.startswith('R'):
        return wordnet.ADV
    else:
        return ''

def review_to_wordlist(review, remove_stopwords = False):
    """This function converts a text sentence to a sequence of words
    Args:
        review (str): String from reviews csv file
        remove_stopwords (Bool): determines whether to remove stop words from lemmatized text
    Yields:
        words (list): A list of lemmatized words
    Remarks:
        First removes html tags
        Then removes non-letters or numbers
        Next, tokenizes sentence into words
        Finally, lemmatizes words and optionally removes stop words
    """
    # 1 - Removing html tags
    review_text = BeautifulSoup(review).get_text()
    # 2 - Removing non-letter or numbers
    review_text = re.sub("[^A-Za-z0-9]", " ", review_text)
    # 3 - Tokenization of sentence into words
    review_text = nltk.word_tokenize(review_text)
    # 4a - Text Lemmatization
    tagged_words = nltk.pos_tag(review_text)
    words = []
    wordnet_lemmatizer = WordNetLemmatizer()
    for w in tagged_words:
        wordnettag = get_wordnet_pos(w[1])
        if wordnettag == "":
            lemmatizedword = wordnet_lemmatizer.lemmatize(w[0].lower())
        else:
            lemmatizedword = wordnet_lemmatizer.lemmatize(w[0].lower(), pos = wordnettag)
        words.append(lemmatizedword.lower())
    # 4b - Optionally remove stopwords
    if remove_stopwords:
        stops = set(stopwords.words("english"))
        words = [w for w in words if not w in stops]
    return(words)
    
def review_to_sentence(review, tokenizer, remove_stopwords = False):
    """# Function that splits a review into sentences
    Args:
        review (str): String from reviews csv file
        tokenizer (nltk tokenizer): Tokenizer from nltk
        remove_stopwords (bool): Determines whether to remove stopwords
    """
    # 1. Using nltk tokenizer
    sentences_to_process = tokenizer.tokenize(review.strip())
    processed_sentences = []
    # 2. Loop for each sentence
    for sentence in sentences_to_process:
        if len(sentence) > 0:
            sentences.append(review_to_wordlist(sentence, remove_stopwords))
    # This returns the list of lists
    return processed_sentences
\end{minted}
\end{mdframed}

We then read the \texttt{.csv} file and tokenize each review by sentences before removing non-alphabetical characters

\begin{mdframed}[backgroundcolor=shadecolor]
\begin{minted}[fontsize=\footnotesize,breaklines]{python}

df = pd.read_csv("200Reviews.csv", encoding = "ISO-8859-1")
tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')
sentences = []
print("Reviews Count:", len(df["review"]))
count = 0
print("Begin parsing train set")
print("Sentences parsed:", end = " ")
for review in df["review"]:
    sentences += review_to_sentence(review, tokenizer, True)
    count += 1
    print(count, end = " ")
    
print("\nDone!")

\end{minted}
\end{mdframed}

The results of the text processing are as follows:

\begin{lstlisting}
    Reviews Count: 200
    Begin parsing train set
    Sentences parsed: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 
    Done!
\end{lstlisting}

\subsection{Co-occurence matrix}
\mybox{Create the co-occurrence matrix for all the remaining words (after stop words are eliminated), where the window of co-occurrence is 5 on either side of the word.}

\textbf{Response:} We write the following function to generate the co-occurence matrix based on each focus word and a fixed window around it

\begin{mdframed}[backgroundcolor=shadecolor]
\begin{minted}[fontsize=\footnotesize,breaklines]{python}

def cooccurence_matrix(sentences, window_size = 5):
    """Function that creates co-occurence matrix based on window size and the text that has been parsed. Skips if window range is outside sentence.
    Args:
        sentences (list of str): A list of strings corresponding to a tokenized sentence
        window-size (int): Determines the size of the window around the focus word to generate co-occurences
    Yields:
        matrix (list of lists of int): The co-ocurrence matrix with the relevant windows_size
    """
    wordlist = set([word for sentence in sentences for word in sentence])
    size = len(wordlist)
    print(f"[{size}] unique words in corpus")
    result_matrix = pd.DataFrame(np.zeros((size, size)), index = wordlist, columns = wordlist)
    print(f"Now counting co-occurences for [{len(sentences)}] Sentences...\nProgress: ", sep = "", end = "")
    for idx, sentence in enumerate(sentences):
        if idx % 100 == 0:
            print(f"{idx}...", end = " ")
        sentence_length = len(sentence)
        for sentence_index, focus_word in enumerate(sentence):
            # Slide window around each focus word
            for wordpair_index in range(1, window_size + 1):
                # if index is after start of sentence
                if sentence_index - wordpair_index >= 0:
                    result_matrix.loc[focus_word, sentence[int(sentence_index - wordpair_index)]] += 1
                # if index is before end of sentence
                if sentence_index + wordpair_index < sentence_length:
                    result_matrix.loc[focus_word, sentence[int(sentence_index + wordpair_index)]] += 1
    print("\nCo-occurence matrix generated.")
    return result_matrix

\end{minted}
\end{mdframed}

Running the code nets us the following

\begin{mdframed}[backgroundcolor=shadecolor]
\begin{minted}[fontsize=\footnotesize,breaklines]{python}
window_size = 5
matrix = cooccurence_matrix(sentences, window_size)
\end{minted}
\end{mdframed}

\begin{lstlisting}

andrew  frolic  whose  hickox  prison  pull  creativity  \
andrew           0.0     0.0    0.0     0.0     0.0   0.0         0.0   
frolic           0.0     0.0    1.0     0.0     0.0   0.0         0.0   
whose            0.0     1.0    0.0     0.0     0.0   0.0         0.0   
hickox           0.0     0.0    0.0     0.0     0.0   0.0         0.0   
prison           0.0     0.0    0.0     0.0     0.0   0.0         0.0   
...              ...     ...    ...     ...     ...   ...         ...   
misanthropic     0.0     0.0    0.0     0.0     0.0   0.0         0.0   
playing          0.0     0.0    0.0     0.0     0.0   0.0         0.0   
depicts          0.0     0.0    0.0     0.0     0.0   0.0         0.0   
filthy           0.0     0.0    0.0     0.0     0.0   0.0         0.0   
moody            0.0     0.0    0.0     0.0     0.0   0.0         0.0   

                circumstance  rabies  1986  ...  herein  kiss  decides  \
andrew                 0.0     0.0   0.0  ...     0.0   0.0      0.0   
frolic                 0.0     0.0   0.0  ...     0.0   0.0      0.0   
whose                  0.0     0.0   0.0  ...     0.0   0.0      0.0   
hickox                 0.0     0.0   0.0  ...     0.0   0.0      0.0   
prison                 0.0     0.0   0.0  ...     0.0   0.0      0.0   
...                    ...     ...   ...  ...     ...   ...      ...   
misanthropic           0.0     0.0   0.0  ...     0.0   0.0      0.0   
playing                0.0     0.0   0.0  ...     0.0   0.0      0.0   
depicts                0.0     0.0   0.0  ...     0.0   0.0      0.0   
filthy                 0.0     0.0   0.0  ...     0.0   0.0      0.0   
moody                  0.0     0.0   0.0  ...     0.0   0.0      0.0   

                conclude  uneven  misanthropic  playing  depicts  filthy  moody  
andrew             0.0     0.0           0.0      0.0      0.0     0.0    0.0  
frolic             0.0     0.0           0.0      0.0      0.0     0.0    0.0  
whose              0.0     0.0           0.0      0.0      0.0     0.0    0.0  
hickox             0.0     0.0           0.0      0.0      0.0     0.0    0.0  
prison             0.0     0.0           0.0      0.0      0.0     0.0    0.0  
...                ...     ...           ...      ...      ...     ...    ...  
misanthropic       0.0     0.0           0.0      0.0      0.0     0.0    0.0  
playing            0.0     0.0           0.0      0.0      0.0     0.0    0.0  
depicts            0.0     0.0           0.0      0.0      0.0     0.0    0.0  
filthy             0.0     0.0           0.0      0.0      0.0     0.0    0.0  
moody              0.0     0.0           0.0      0.0      0.0     0.0    0.0  

[6027 rows x 6027 columns]
\end{lstlisting}

\subsection{Word embeddings}
\mybox{Apply SVD and obtain word embeddings of size 100.}

\textbf{Response:} We now import of packages used for Singular Value Decomposition of our co-occurence matrix from the previous section

To reduce our dimensionality, we limit our word embedding size to 100.

\begin{mdframed}[backgroundcolor=shadecolor]
\begin{minted}[fontsize=\footnotesize,breaklines]{python}
from scipy.linalg import svd
from numpy import diag
U, s, VT = svd(matrix)
embedding_size = 100
U = U[:, :embedding_size]
S = S[:embedding_size, :embedding_size]
VT = VT[:embedding_size, :]

print("Size of new matrix U is", U.shape)
print("Size of new matrix S is", S.shape)
print("Size of new matrix VT is", VT.shape)

embedding_matrix = pd.DataFrame(U, index = matrix.index)
\end{minted}
\end{mdframed}

Which nets us Matrix $U$ with dimensions $6027\times 100$

\begin{lstlisting}
Size of new matrix U is (6027, 100)
Size of new matrix S is (100, 100)
Size of new matrix VT is (100, 6027)
\end{lstlisting}

\subsection{Word2Vec}

\mybox{Then, please generate word embeddings of size 100 using \texttt{Word2Vec.pynb} (from the NLP lecture) on the same "\texttt{200Reviews.csv}" dataset. 
Please show comparison on few examples to understand which method works better. Note your observations in your solution.}

\subsubsection{Code section}

\textbf{Response:}

\begin{mdframed}[backgroundcolor=shadecolor]
\begin{minted}[fontsize=\footnotesize,breaklines]{python}
# Creating the model and setting values for the various parameters
# Using the same sentences that are input into the model to maintain comparability
num_features = 100  # Word vector dimensionality
min_word_count = 1 # Minimum word count
num_workers = 4     # Number of parallel threads
context = 5        # Context window size
downsampling = 1e-3 # (0.001) Downsample setting for frequent words

# Initializing the train model
from gensim.models import word2vec
print("Training model....")
model = word2vec.Word2Vec(sentences,\
                          workers = num_workers,\
                          vector_size = num_features,\
                          min_count = min_word_count,\
                          window = context,
                          sample = downsampling)
print(model)
print("Done.")
\end{minted}
\end{mdframed}

We write the following functions to aid in determining how similar word are to one another.

\begin{mdframed}[backgroundcolor=shadecolor]
\begin{minted}[fontsize=\footnotesize,breaklines]{python}
def cosine_similarity(word1, word2):
    """Calculates cosine similarity between two 2 word dimension features
    Args:
        word1 (list or pandas Series)
        word2 (list or pandas Series)

    Yields:
        cosine_similarity (0 < float < 1): Cosine similarity
    """
    if (np.linalg.norm(word1)*np.linalg.norm(word2)) == 0:
        cosine_similarity = -100
    else:
        cosine_similarity = np.dot(word1, word2) / (np.linalg.norm(word1)*np.linalg.norm(word2))
    return cosine_similarity


def most_similar(matrix, target_word, number=10):
    """Prints out the a number of most similar words based on cosine similarity
    Args:
        matrix (pandas DataFrame): Word embedding matrix
        word (str): Word vector to index the matrix
        number (int): How many words to return
    Yields:
        None 
    """
    print(f"Target word is [{target_word}]")
    features = np.array(matrix)
    word_features = np.array(matrix.loc[target_word])
    feature_cos_similarities = []
    for feature in features:
        feature_cos_similarities.append(cosine_similarity(feature, word_features))
    print("-"*56)
    
    similarity_rankings = np.argsort(feature_cos_similarities)
    for i in range(1, number+2):
        if matrix.index[similarity_rankings[-i]] != target_word:
            print(i-1, matrix.index[similarity_rankings[-i]], feature_cos_similarities[similarity_rankings[-i]])
    print("-"*56)
    
def doesnt_match(matrix, words):
    """Prints out the word that does not match in a list of words
    Args:
        matrix (pandas DataFrame): Word embedding matrix
        words (list of str): list of words to determine odd word
    Yields:
        None
    Remarks:
        Iterates through each focus word in words and calculates cosine_similarity between the focus word and the other words.
        The occurrence most dissimilar word from each focus word is added to a dict which tracks the number of occurences
        The word with the highest value in the dict is the word which is determined to be dissimilar from the most number of focus words.
    """
    oddwords = {}
    for focus_word in words:
        focus_word_features = np.array(matrix.loc[focus_word])
        similarity_list = []
        for compared_word in words:
            compared_word_features = np.array(matrix.loc[compared_word])
            similarity_list.append(cosine_similarity(compared_word_features, focus_word_features))
        odd_word = words[np.argsort(similarity_list)[0]]

        if odd_word in oddwords:
            oddwords[odd_word]+=1
        else:
            oddwords[odd_word]=1
    
    result = max(oddwords.keys(), key=oddwords.get)
    print(f"Odd Word: {result}")
\end{minted}
\end{mdframed}

We apply above functions by iterating through our test case.

\begin{mdframed}[backgroundcolor=shadecolor]
\begin{minted}[fontsize=\footnotesize,breaklines]{python}

# Words that will be tested on
similarwords = ["thrill", "horror", "comedy", "romance"]

for word in similarwords:
    print("Most Similar Words based on SVD Count Based Method")
    most_similar(embedding_matrix, word)
    print("Most Similar Words based on Word2Vec")
    print("Target Word:", word)
    count = 1
    for word2, score in model.wv.most_similar(word):
        print(count, word2, score)
        count += 1
    print("="*56)
    print()
\end{minted}
\end{mdframed}

\subsubsection{Comparison between \texttt{Word2Vec} and \texttt{SVD}}

{\large Word Similarity}

Both techniques give mixed results for word similarity. 

For instance, the word "horror" is correctly associated with Edgar Allen Poe using our \texttt{SVD} trained model. However, the rest of the words do not seem to have much bearing. 

\texttt{Word2Vec} returns much more vague results, with words like "film" and "character" being the closest associated words from my personal opinion.

For "romance" the results are even worse. "romance" was even associated with "zombie" for our \texttt{Word2Vec} case.

In conclusion, both approaches perform differently for different words as evidenced by the difference in scores between each approach on the same word.

Our results for word similarity are as follows:

\begin{lstlisting}
Most Similar Words based on SVD Count Based Method
Target word is [thrill]
--------------------------------------------------------
0 overjoy 1.0
2 weep 0.9046623917319988
3 wiz 0.878417848436146
4 alternative 0.853546730780722
5 grass 0.853546730780722
6 fascinating 0.8470791202164661
7 ceremony 0.8435123498010818
8 caribbean 0.7972308062841066
9 starz 0.7943825761227766
10 worried 0.7907181622775727
--------------------------------------------------------
Most Similar Words based on Word2Vec
Target Word: thrill
1 fossilized 0.3384301960468292
2 cause 0.318146288394928
3 sitcom 0.3180883526802063
4 extremely 0.3119494915008545
5 miserable 0.3063412606716156
6 seek 0.30342936515808105
7 suggest 0.29555368423461914
8 coalition 0.292111873626709
9 tight 0.2920524775981903
10 thumb 0.2899966239929199
========================================================

Most Similar Words based on SVD Count Based Method
Target word is [horror]
--------------------------------------------------------
1 edgar 0.6768376528294273
2 interview 0.6651176540690175
3 allan 0.6470243359005875
4 panacea 0.6407844605771518
5 wager 0.6300636665392728
6 lock 0.6166518351521574
7 helicopter 0.6061206116425559
8 foster 0.5987050637694958
9 anthology 0.5944347845125878
10 cassandra 0.5847528162847823
--------------------------------------------------------
Most Similar Words based on Word2Vec
Target Word: horror
1 go 0.8576502203941345
2 like 0.8498179912567139
3 film 0.8475027084350586
4 one 0.846524715423584
5 think 0.8419342041015625
6 way 0.8391382694244385
7 character 0.8364621996879578
8 get 0.835287868976593
9 well 0.8350684642791748
10 see 0.8343316316604614
========================================================

Most Similar Words based on SVD Count Based Method
Target word is [comedy]
--------------------------------------------------------
1 screwball 0.7250405827707767
2 wit 0.7136601982377639
3 operate 0.7136601982377637
4 vein 0.6818061612483495
5 hawn 0.6398410238592813
6 spirited 0.636001653583391
7 thoroughly 0.6274431698054069
8 beneath 0.6203970575781339
9 surface 0.6203970575781339
10 parable 0.6068151913645826
--------------------------------------------------------
Most Similar Words based on Word2Vec
Target Word: comedy
1 make 0.7479692697525024
2 go 0.7388730049133301
3 bad 0.7355589270591736
4 film 0.7346001863479614
5 movie 0.7264123558998108
6 seem 0.7254176139831543
7 get 0.7252454161643982
8 try 0.7238624691963196
9 watch 0.7223445177078247
10 show 0.7219111323356628
========================================================

Most Similar Words based on SVD Count Based Method
Target word is [romance]
--------------------------------------------------------
1 tabu 0.5293631301273979
2 puberty 0.514623569746616
3 34 0.512576568231544
4 aged 0.4777813093573541
5 yr 0.46953031469126016
6 greenquist 0.45238895365555926
7 cursed 0.44220470762691644
8 learn 0.4360842118936542
9 engineer 0.4284360059933918
10 compact 0.42566218513482196
--------------------------------------------------------
Most Similar Words based on Word2Vec
Target Word: romance
1 year 0.41749727725982666
2 make 0.4086826741695404
3 low 0.40472960472106934
4 much 0.3980807363986969
5 zombie 0.3898499608039856
6 night 0.3881601095199585
7 give 0.3855125904083252
8 split 0.38508740067481995
9 know 0.3837257921695709
10 support 0.38248103857040405
========================================================
    
\end{lstlisting}

{\large Odd Word Out}

In terms of "odd word out" applications, our \texttt{SVD} seemed to perform better for the second test case.

It correctly determined that man was not associated with the other animals, whereas \texttt{Word2Vec} chose "lion"

Our results are as follows:

\begin{mdframed}[backgroundcolor=shadecolor]
\begin{minted}[fontsize=\footnotesize,breaklines]{python}
doesntmatch = ["man woman dog child kitchen", 
               "cat dog lion tiger man"]

for words in doesntmatch:
    print("List of Words:", words)
    print("SVD ", end = "")
    doesnt_match(embedding_matrix, words.split())
    print("Word2Vec Odd Word:", model.wv.doesnt_match(words.split()))
    print()

\end{minted}
\end{mdframed}

\begin{lstlisting}
List of Words: man woman dog child kitchen
SVD Odd Word: kitchen
Word2Vec Odd Word: kitchen

List of Words: cat dog lion tiger man
SVD Odd Word: man
Word2Vec Odd Word: lion    
\end{lstlisting}

\end{document}
