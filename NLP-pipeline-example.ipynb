{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9-final"},"colab":{"name":"NLP-pipeline-example.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"Ela_9oRqX181"},"source":["# Building an NLP Pipeline, Step-by-Step\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8tyLW8uJX185"},"source":["### Note: It’s worth mentioning that these are the steps in a typical NLP pipeline, but you will skip steps or re-order steps depending on what you want to do and how your NLP library is implemented.\n","\n","Let’s look at a piece of text from Wikipedia:\n","\n","London is the capital and most populous city of England and the United Kingdom. Standing on the River Thames in the south east of the island of Great Britain, London has been a major settlement for two millennia. It was founded by the Romans, who named it Londinium.\n","\n","(Source: Wikipedia article “London”)\n","\n","This paragraph contains several useful facts. It would be great if a computer could read this text and understand that London is a city, London is located in England, London was settled by Romans and so on. But to get there, we have to first teach our computer the most basic concepts of written language and then move up from there."]},{"cell_type":"code","metadata":{"id":"dTsiz_77X187"},"source":["import nltk\n","from nltk.stem import WordNetLemmatizer\n","import sys\n","from nltk.corpus import wordnet\n","from nltk.corpus import stopwords\n","from nltk.parse.malt import MaltParser\n","import pandas as pd\n","import seaborn as sns\n","import numpy as np"],"execution_count":5,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["mp = MaltParser(\"maltparser-1.8.1\", \"engmalt.linear-1.7.mco\")"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["     Unnamed: 0          id  sentiment  \\\n","0             0    \"5814_8\"          1   \n","1             1    \"2381_9\"          1   \n","2             2    \"7759_3\"          0   \n","3             3    \"3630_4\"          0   \n","4             4    \"9495_8\"          1   \n","..          ...         ...        ...   \n","195         195    \"8807_9\"          1   \n","196         196  \"12148_10\"          1   \n","197         197   \"10771_2\"          0   \n","198         198    \"6766_3\"          0   \n","199         199   \"2072_10\"          1   \n","\n","                                                review  \n","0    \"With all this stuff going down at the moment ...  \n","1    \"\\\"The Classic War of the Worlds\\\" by Timothy ...  \n","2    \"The film starts with a manager (Nicholas Bell...  \n","3    \"It must be assumed that those who praised thi...  \n","4    \"Superbly trashy and wondrously unpretentious ...  \n","..                                                 ...  \n","195  \"This is a collection of documentaries that la...  \n","196  \"This movie has a lot of comedy, not dark and ...  \n","197  \"Have not watched kids films for some years, s...  \n","198  \"You probably heard this phrase when it come t...  \n","199  \"I was about thirteen when this movie came out...  \n","\n","[200 rows x 4 columns]"],"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>id</th>\n      <th>sentiment</th>\n      <th>review</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>\"5814_8\"</td>\n      <td>1</td>\n      <td>\"With all this stuff going down at the moment ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>\"2381_9\"</td>\n      <td>1</td>\n      <td>\"\\\"The Classic War of the Worlds\\\" by Timothy ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>\"7759_3\"</td>\n      <td>0</td>\n      <td>\"The film starts with a manager (Nicholas Bell...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>\"3630_4\"</td>\n      <td>0</td>\n      <td>\"It must be assumed that those who praised thi...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>\"9495_8\"</td>\n      <td>1</td>\n      <td>\"Superbly trashy and wondrously unpretentious ...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>195</th>\n      <td>195</td>\n      <td>\"8807_9\"</td>\n      <td>1</td>\n      <td>\"This is a collection of documentaries that la...</td>\n    </tr>\n    <tr>\n      <th>196</th>\n      <td>196</td>\n      <td>\"12148_10\"</td>\n      <td>1</td>\n      <td>\"This movie has a lot of comedy, not dark and ...</td>\n    </tr>\n    <tr>\n      <th>197</th>\n      <td>197</td>\n      <td>\"10771_2\"</td>\n      <td>0</td>\n      <td>\"Have not watched kids films for some years, s...</td>\n    </tr>\n    <tr>\n      <th>198</th>\n      <td>198</td>\n      <td>\"6766_3\"</td>\n      <td>0</td>\n      <td>\"You probably heard this phrase when it come t...</td>\n    </tr>\n    <tr>\n      <th>199</th>\n      <td>199</td>\n      <td>\"2072_10\"</td>\n      <td>1</td>\n      <td>\"I was about thirteen when this movie came out...</td>\n    </tr>\n  </tbody>\n</table>\n<p>200 rows × 4 columns</p>\n</div>"},"metadata":{},"execution_count":6}],"source":["df = pd.read_csv(\"200Reviews.csv\")\n","df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# paragraph = \"London is the capital and most populous city of England and the United Kingdom.\"\\\n","#              \" Standing on the River Thames in the south east of the island of Great Britain,\"\\\n","#              \"London has been a major settlement for two millennia. It was founded by the Romans, who named it Londinium.\"\n","# print(\"Paragraph considered is:\")\n","# print(paragraph)"]},{"cell_type":"markdown","metadata":{"id":"EZaRGQqvX188"},"source":["## Step 1: Sentence Segmentation\n","The first step in the pipeline is to break the text apart into separate sentences. "]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0      [\"With all this stuff going down at the moment...\n","1      [\"\\\"The Classic War of the Worlds\\\" by Timothy...\n","2      [\"The film starts with a manager (Nicholas Bel...\n","3      [\"It must be assumed that those who praised th...\n","4      [\"Superbly trashy and wondrously unpretentious...\n","                             ...                        \n","195    [\"This is a collection of documentaries that l...\n","196    [\"This movie has a lot of comedy, not dark and...\n","197    [\"Have not watched kids films for some years, ...\n","198    [\"You probably heard this phrase when it come ...\n","199    [\"I was about thirteen when this movie came ou...\n","Name: tokenized, Length: 200, dtype: object"]},"metadata":{},"execution_count":9}],"source":["df[\"tokenized\"] = df['review'].map(nltk.sent_tokenize)\n","df['tokenized']"]},{"cell_type":"markdown","metadata":{"id":"m8K5bSlvX189"},"source":["## Step 2: Word Tokenization\n","Now that we’ve split our document into sentences, we can process them one at a time. The next step in our pipeline is to \n","break this sentence into separate words or tokens. This is called tokenization. "]},{"cell_type":"code","metadata":{"id":"PCYp_6pOX189"},"source":["#Step 2: Word tokenization\n","\n","df[\"word_list\"] = df['tokenized'].map(nltk.word_tokenize)\n","df[\"word_list\"]\n","\n","### TODO word tokenize pipeline\n","\n","# for k in range(len(sentences)):\n","#     # word tokenizer will keep the punctuations. To get rid of punctuations, use nltk.RegexpTokenizer(r'\\w+').tokenize(sentences[k]) \n","#     words = nltk.word_tokenize(sentences[k])\n","#     print(\"Words in sentence \"+repr(k+1)+\" are: \")\n","#     wordlist=[]\n","#     for w in words:\n","#         wordlist.append(w)\n","#     print(wordlist)"],"execution_count":12,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"expected string or bytes-like object","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[1;32m<ipython-input-12-1c31dabd71b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Step 2: Word tokenization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"word_list\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tokenized'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"word_list\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# for k in range(len(sentences)):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mC:\\tf\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, arg, na_action)\u001b[0m\n\u001b[0;32m   3904\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3905\u001b[0m         \"\"\"\n\u001b[1;32m-> 3906\u001b[1;33m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mna_action\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mna_action\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3907\u001b[0m         return self._constructor(new_values, index=self.index).__finalize__(\n\u001b[0;32m   3908\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"map\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mC:\\tf\\lib\\site-packages\\pandas\\core\\base.py\u001b[0m in \u001b[0;36m_map_values\u001b[1;34m(self, mapper, na_action)\u001b[0m\n\u001b[0;32m    935\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    936\u001b[0m         \u001b[1;31m# mapper is a function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 937\u001b[1;33m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmap_f\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    938\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    939\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnew_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mpandas\\_libs\\lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n","\u001b[1;32mC:\\tf\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[1;33m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    128\u001b[0m     \"\"\"\n\u001b[1;32m--> 129\u001b[1;33m     \u001b[0msentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    130\u001b[0m     return [\n\u001b[0;32m    131\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mC:\\tf\\lib\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    105\u001b[0m     \"\"\"\n\u001b[0;32m    106\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tokenizers/punkt/{0}.pickle\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mC:\\tf\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1270\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m         \"\"\"\n\u001b[1;32m-> 1272\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1274\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mC:\\tf\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1324\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m         \"\"\"\n\u001b[1;32m-> 1326\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1328\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mC:\\tf\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1324\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m         \"\"\"\n\u001b[1;32m-> 1326\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1328\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mC:\\tf\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1314\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m             \u001b[0mslices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1316\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0msl\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1317\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mC:\\tf\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[1;34m(self, text, slices)\u001b[0m\n\u001b[0;32m   1355\u001b[0m         \"\"\"\n\u001b[0;32m   1356\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1357\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0msl1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msl2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1358\u001b[0m             \u001b[0msl1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msl1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msl1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1359\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msl2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mC:\\tf\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[1;34m(it)\u001b[0m\n\u001b[0;32m    312\u001b[0m     \u001b[0mit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 314\u001b[1;33m         \u001b[0mprev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32mC:\\tf\\lib\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1328\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1330\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1331\u001b[0m             \u001b[0mcontext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"after_tok\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1332\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"]}]},{"cell_type":"markdown","metadata":{"id":"24qi51IdX189"},"source":["## Step 3: Predicting Parts of Speech for Each Token\n","Next, we’ll look at each token and try to guess its part of speech — whether it is a noun, a verb, an adjective and so on. Knowing the role of each word in the sentence will help us start to figure out what the sentence is talking about.\n","\n","We can do this by feeding each word (and some extra words around it for context) into a pre-trained part-of-speech classification model:\n","\n","\n","The part-of-speech model was originally trained by feeding it millions of English sentences with each word’s part of speech already tagged and having it learn to replicate that behavior."]},{"cell_type":"code","metadata":{"id":"RWfeIG6DX18-"},"source":["#Step 3: Predicting parts off speech for each token\n","# You can use nltk.help.upenn_tagset() to get the description of each of pos tag.\n","#Uncomment following line to print the list of all tags\n","#nltk.download('tagsets')\n","#nltk.help.upenn_tagset()\n","for k in range(len(sentences)):\n","    words = nltk.word_tokenize(sentences[k])\n","    tagged_words = nltk.pos_tag(words)\n","    print(\"Tagged Words in sentence \"+repr(k+1)+\" are: \")\n","    print(tagged_words)\n","   "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nRzZPXyKX18-"},"source":["## Step 4: Text Lemmatization\n","In English (and most languages), words appear in different forms. Look at these two sentences:\n","\n","I had a pony.\n","\n","I had two ponies.\n","\n","Both sentences talk about the noun pony, but they are using different inflections. When working with text in a computer, it is helpful to know the base form of each word so that you know that both sentences are talking about the same concept. Otherwise the strings “pony” and “ponies” look like two totally different words to a computer.\n","\n","In NLP, we call finding this process lemmatization — figuring out the most basic form or lemma of each word in the sentence."]},{"cell_type":"code","metadata":{"id":"Sw3pG3ElX18-"},"source":["#Step 4: Text Lemmatization\n","#As we are using wordnet Lemmatizer and the the standard NLTK pos tags are treebank tags, we need to convert the treebank tag\n","#to wordnet tags. \n","def get_wordnet_pos(treebank_tag):\n","\n","    if treebank_tag.startswith('J'):\n","        return wordnet.ADJ\n","    elif treebank_tag.startswith('V'):\n","        return wordnet.VERB\n","    elif treebank_tag.startswith('N'):\n","        return wordnet.NOUN\n","    elif treebank_tag.startswith('R'):\n","        return wordnet.ADV\n","    else:\n","        return ''\n","    \n","wordnet_lemmatizer = WordNetLemmatizer()\n","for k in range(len(sentences)):\n","    words = nltk.word_tokenize(sentences[k])\n","    tagged_words = nltk.pos_tag(words)\n","    lemmatized_wordlist=[]\n","    print(\"Word:Lemmatized Word in sentence \"+repr(k+1)+\" are: \")\n","    for w in tagged_words:\n","        wordnettag=get_wordnet_pos(w[1])\n","        if wordnettag == '':\n","            lemmatizedword = wordnet_lemmatizer.lemmatize(w[0].lower())\n","        else:\n","            lemmatizedword = wordnet_lemmatizer.lemmatize(w[0].lower(),pos=wordnettag)\n","        if w[0].istitle():\n","            lemmatizedword = lemmatizedword.capitalize()\n","        elif w[0].upper()==w[0]:\n","            lemmatizedword = lemmatizedword.upper()\n","        else:\n","            lemmatizedword = lemmatizedword\n","        lemmatized_wordlist.append((w[0],lemmatizedword))\n","            \n","    print(lemmatized_wordlist)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZiQZpEcYX18_"},"source":["## Step 5: Identifying Stop Words\n","Next, we want to consider the importance of a each word in the sentence. English has a lot of filler words that appear very frequently like “and”, “the”, and “a”. When doing statistics on text, these words introduce a lot of noise since they appear way more frequently than other words. Some NLP pipelines will flag them as stop words —that is, words that you might want to filter out before doing any statistical analysis."]},{"cell_type":"code","metadata":{"id":"j6502yIIX18_"},"source":["#Step 5: Identifying stop words\n","stopWords = set(stopwords.words('english'))\n","for k in range(len(sentences)):\n","    words = nltk.word_tokenize(sentences[k])\n","    wordlist_wo_stopwords=[]\n","    print(\"Words in sentence \"+repr(k+1)+\" without stop words are: \")\n","    for w in words:\n","        if w not in stopWords:\n","            wordlist_wo_stopwords.append(w)\n","    print(wordlist_wo_stopwords)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aokFkgGqX19A"},"source":["## Step 6a: Dependency Parsing\n","The next step is to figure out how all the words in our sentence relate to each other. This is called dependency parsing.\n","\n","The goal is to build a tree that assigns a single parent word to each word in the sentence. The root of the tree will be the main verb in the sentence. Here’s what the beginning of the parse tree will look like for our sentence:"]},{"cell_type":"code","metadata":{"id":"_ZDfu9oGX19B"},"source":["#!wget http://www.maltparser.org/mco/english_parser/engmalt.linear-1.7.mco\n","#Step 6a: Dependency parsing\n","from nltk.parse.malt import MaltParser\n","from nltk.tree import Tree\n","from nltk.draw.tree import TreeView\n","import sys\n","sys.path.insert(0,'./maltparser-1.8.1')\n","sys.path.insert(0,'.')\n","#Malt parser works on sentences. It internally performs tokenization, pos tagging etc. \n","#mp = MaltParser('maltparser-1.8.1', 'engmalt.linear-1.7.mco')\n","trees=[]\n","for k in range(len(sentences)):\n","    trees.append(mp.parse_one(sentences[k].split()).tree())\n","for k in range(len(trees)):\n","    print(\"Dependency tree for sentence \"+repr(k+1))\n","    print(trees[k])\n","    #Uncomment the following to visualize the tree\n","    #print(trees[k].draw())\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LAtKx5DNX19C"},"source":["## Step 6b: Finding Noun Phrases\n","Sometimes it makes more sense to group together the words that represent a single idea or thing. We can use the information from the dependency parse tree to automatically group together words that are all talking about the same thing."]},{"cell_type":"code","metadata":{"id":"eq8DrB8xX19C"},"source":["#Step 6b: Finding noun phrases\n","#In the above step we used the malt\n","grammar = \"\"\"NP: {<DT>?<JJ>*<NN.*>+}\n","       RELATION: {<V.*>}\n","                 {<DT>?<JJ>*<NN.*>+}\n","       ENTITY: {<NN.*>}\"\"\"\n","\n","cp = nltk.RegexpParser(grammar)\n","for k in range(len(sentences)):\n","    words = nltk.word_tokenize(sentences[k])\n","    tagged_words = nltk.pos_tag(words)\n","    lemmatized_wordlist=[]\n","    print(\"Noun phrases in sentence \"+repr(k+1)+\" are: \")\n","    for w in tagged_words:\n","        wordnettag=get_wordnet_pos(w[1])\n","        if wordnettag == '':\n","            lemmatizedword = wordnet_lemmatizer.lemmatize(w[0].lower())\n","        else:\n","            lemmatizedword = wordnet_lemmatizer.lemmatize(w[0].lower(),pos=wordnettag)\n","        if w[0].istitle():\n","            lemmatizedword = lemmatizedword.capitalize()\n","        elif w[0].upper()==w[0]:\n","            lemmatizedword = lemmatizedword.upper()\n","        else:\n","            lemmatizedword = lemmatizedword\n","        lemmatized_wordlist.append((lemmatizedword,w[1]))\n","            \n","   # print(lemmatized_wordlist)\n","\n","    noun_phrases_list = [' '.join(leaf[0] for leaf in tree.leaves()) \n","                      for tree in cp.parse(lemmatized_wordlist).subtrees() \n","                      if tree.label()=='NP'] \n","    result = cp.parse(lemmatized_wordlist)\n","    #print(result)\n","    #print(type(result))\n","    \n","    print(noun_phrases_list)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uQlGJzknX19D"},"source":["## Step 7. Named Entity Recognition (NER)\n","The goal of Named Entity recognition is to detect and label nouns with real world concepts they represent. But Named Entity Recgnition systems aren’t just doing a simple dictionary lookup. Instead,they are using the context of how a word appears in the sentence and a statistical model to guess which type of noun a word represents.\n"]},{"cell_type":"code","metadata":{"id":"ABvJoMauX19D"},"source":["#Step 7. Named Entity Recognition (NER)\n","nltk.download('words')\n","nltk.download('maxent_ne_chunker')\n","\"\"\"\n","geo = Geographical Entity\n","org = Organization\n","per = Person\n","gpe = Geopolitical Entity\n","tim = Time indicator\n","art = Artifact\n","eve = Event\n","nat = Natural Phenomenon\n","\n","\"\"\"\n","for k in range(len(sentences)):\n","    words = nltk.word_tokenize(sentences[k])\n","    tagged_words = nltk.pos_tag(words)\n","    ne_tagged_words = nltk.ne_chunk(tagged_words)\n","    #print(ne_tagged_words)\n","    print(\"Named entities in  sentence \"+repr(k+1))\n","    for chunk in ne_tagged_words:\n","        if hasattr(chunk, 'label'):\n","            print(chunk.label(), ' '.join(c[0] for c in chunk))\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eg1q208VX19E"},"source":["Notice that it makes some mistakes. This is probably because there was nothing in the training data set similar to that and it made a best guess."]},{"cell_type":"markdown","metadata":{"id":"ZlNfBDsRX19E"},"source":["## Step 8: Coreference Resolution\n","At this point, we already have a useful representation of our sentence. We know the parts of speech for each word, how the words relate to each other and which words are talking about named entities. However, we still have one big problem. English is full of pronouns — words like he, she, and it. These are shortcuts that we use instead of writing out names over and over in each sentence. Humans can keep track of what these words represent based on context. But our NLP model doesn’t know what pronouns mean because it only examines one sentence at a time.\n","\n","Let’s look at the third sentence in our paragraph:“It was founded by the Romans, who named it Londinium.”\n","\n","As a human reading this sentence, you can easily Dgure out that “it” means “London”. The goal of coreference resolution is to Dgure out this same mapping by tracking pronouns across sentences. We want to figure out all the words that are referring to the same entity.\n"]},{"cell_type":"code","metadata":{"id":"pOWTQ-AtX19E"},"source":["#Step 8: Coreference resolution\n","#Check out this great coreference resolution demo from Hugging Face. https://huggingface.co/coref/ "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pBguCoieX19E"},"source":[],"execution_count":null,"outputs":[]}]}